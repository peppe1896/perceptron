{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Perceptron votato*\n",
    "\n",
    "### Imports - Prime variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Imports #\n",
    "###########\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn import datasets as ddd\n",
    "import sklearn\n",
    "\n",
    "###################\n",
    "# Prime variabili #\n",
    "###################\n",
    "\n",
    "file_path = \"Dataset/mushrooms.csv\"                 # Dataset\n",
    "df_complete = pd.read_csv(file_path)                # Seleziona CSV - Tutto il Dataset in file_path\n",
    "df_complete.dropna()\n",
    "df_complete = df_complete.head(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
      "0        p         x           s         n       t    p               f   \n",
      "1        e         x           s         y       t    a               f   \n",
      "2        e         b           s         w       t    l               f   \n",
      "3        p         x           y         w       t    p               f   \n",
      "4        e         x           s         g       f    n               f   \n",
      "...    ...       ...         ...       ...     ...  ...             ...   \n",
      "8119     e         k           s         n       f    n               a   \n",
      "8120     e         x           s         n       f    n               a   \n",
      "8121     e         f           s         n       f    n               a   \n",
      "8122     p         k           y         n       f    y               f   \n",
      "8123     e         x           s         n       f    n               a   \n",
      "\n",
      "     gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
      "0               c         n          k  ...                        s   \n",
      "1               c         b          k  ...                        s   \n",
      "2               c         b          n  ...                        s   \n",
      "3               c         n          n  ...                        s   \n",
      "4               w         b          k  ...                        s   \n",
      "...           ...       ...        ...  ...                      ...   \n",
      "8119            c         b          y  ...                        s   \n",
      "8120            c         b          y  ...                        s   \n",
      "8121            c         b          n  ...                        s   \n",
      "8122            c         n          b  ...                        k   \n",
      "8123            c         b          y  ...                        s   \n",
      "\n",
      "     stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
      "0                         w                      w         p          w   \n",
      "1                         w                      w         p          w   \n",
      "2                         w                      w         p          w   \n",
      "3                         w                      w         p          w   \n",
      "4                         w                      w         p          w   \n",
      "...                     ...                    ...       ...        ...   \n",
      "8119                      o                      o         p          o   \n",
      "8120                      o                      o         p          n   \n",
      "8121                      o                      o         p          o   \n",
      "8122                      w                      w         p          w   \n",
      "8123                      o                      o         p          o   \n",
      "\n",
      "     ring-number ring-type spore-print-color population habitat  \n",
      "0              o         p                 k          s       u  \n",
      "1              o         p                 n          n       g  \n",
      "2              o         p                 n          n       m  \n",
      "3              o         p                 k          s       u  \n",
      "4              o         e                 n          a       g  \n",
      "...          ...       ...               ...        ...     ...  \n",
      "8119           o         p                 b          c       l  \n",
      "8120           o         p                 b          v       l  \n",
      "8121           o         p                 b          c       l  \n",
      "8122           o         e                 w          v       l  \n",
      "8123           o         p                 o          c       l  \n",
      "\n",
      "[8124 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Dataframe intero #\n",
    "####################\n",
    "columns_to_drop = []\n",
    "df_complete = df_complete.drop(columns=columns_to_drop)\n",
    "print(df_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni - Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Funzioni per il dataset #\n",
    "###########################\n",
    "\n",
    "# Crea la mappa degli attributi, che serve per capire se un attributo è positivo o negativo\n",
    "# Se si passa map_json, vuol dire che si è già ricomposto un json precedentemente salvato, e quindi non serve ricrearlo\n",
    "# Una versione incapsulata è quella dopo, ovvero get_attribute_map_from_json\n",
    "\n",
    "def generate_attributes_map(map_json=[], dataframe=None, save_json=False, name=\"DELETABLE\", numeric_dataset=False, standardize=False):\n",
    "    map_of_attributes = map_json\n",
    "    if dataframe is None and len(map_of_attributes)==0:\n",
    "        return \"error - bad call\"\n",
    "    if len(map_of_attributes) == 0 :\n",
    "        for col in list(dataframe.columns.values):\n",
    "            if numeric_dataset:\n",
    "                column_to_array = dataframe[col].to_numpy()\n",
    "                max = float(dataframe[col].max())\n",
    "                min = float(dataframe[col].min())\n",
    "                mid_unstandardized = np.mean(column_to_array)                                   #(max + min) / 2\n",
    "                if standardize:\n",
    "                    mean = float(np.mean(column_to_array))\n",
    "                    std = float(np.std(column_to_array))\n",
    "                    for i in range(len(dataframe[col])):\n",
    "                        dataframe.at[i, col] = (dataframe.at[i, col] - mean) / std              # Eseguo la standardizzazione - Sotto c'è la Normalizzazione\n",
    "                    new_max = float(dataframe[col].max())                                       # 2 * (dataframe.at[i, col]-min)/(max - min) - 1 \n",
    "                    new_min = float(dataframe[col].min())                \n",
    "                    mid = (new_max + new_min) / 2\n",
    "                else:\n",
    "                    mid = mid_unstandardized\n",
    "                map_of_attributes.append((str(col), \"middle\", [mid, mid_unstandardized]))       # Seleziono un mid per ogni attributo - magg o ugu a mid sono positivi\n",
    "            else:\n",
    "                attributes = df_complete[col].unique()\n",
    "                split = np.array_split(attributes, 2)\n",
    "                map_of_attributes.append((str(col), \"positive\", list(split[0])))\n",
    "                map_of_attributes.append((str(col), \"negative\", list(split[1])))\n",
    "    attributes_map = pd.DataFrame(map_of_attributes, columns=[\"Column\", \"Sign\", \"Attributes\"])\n",
    "\n",
    "    if save_json:\n",
    "        with open(\"Dataset/MappedAttributes/\"+name+\".json\", \"w\") as outfile:\n",
    "            outfile.write(json.dumps(map_of_attributes))\n",
    "    \n",
    "    return attributes_map\n",
    "\n",
    "# Crea un dataframe che contiene tutte le tuple di entry (ovvero tutti i vettori x) che servono per calcolare \n",
    "\n",
    "def map_values(dataframe, attributes_map, numeric_values=False):\n",
    "    df_mapped = pd.DataFrame(columns=dataframe.columns)                                                             # Creo il nuovo dataset con l'insieme di attributi del dataframe originale\n",
    "    if not numeric_values:\n",
    "        for col in dataframe.columns:                                                                               # Per ogni attributo, guardo quale set è definito positivo\n",
    "            positives = list(attributes_map.query(\"Column == '%s' & Sign == 'positive'\" %col )[\"Attributes\"])[0]    # Query nel dataframe per trovare i positivi\n",
    "            negatives = list(attributes_map.query(\"Column == '%s' & Sign == 'negative'\" %col )[\"Attributes\"])[0]\n",
    "            for i in range(len(dataframe)):\n",
    "                if dataframe.loc[i][col] in positives:\n",
    "                    df_mapped.at[i, col] = 1\n",
    "                elif dataframe.loc[i][col] in negatives:\n",
    "                    df_mapped.at[i, col] = -1\n",
    "                else:\n",
    "                    df_mapped.at[i, col] = 0\n",
    "                    print(\"ERROR: This attribute isn't positive or negative!!\" + str(dataframe.loc[i][col]) + \"POSI: \" + str(positives) + \" - NEGA: \" + str(negatives) )\n",
    "                    return\n",
    "    else:\n",
    "        return pd.DataFrame(dataframe)    # Se sono valori numerici, sono stati già normalizzati in generate_attribute_map() - e gli intervalli + e - sono definiti\n",
    "    return df_mapped\n",
    "    \n",
    "def generate_weights(n_attributes, contain_target=True, init_zero=False):\n",
    "    attributes = n_attributes\n",
    "    if contain_target:\n",
    "        attributes -= 1\n",
    "    # attributes -= len(columns_to_drop)              # Bisogna rimuovere anche il numero di colonne rimosse\n",
    "    weights = np.zeros( attributes )\n",
    "    if init_zero:\n",
    "        return weights\n",
    "    len_weights = len(weights)\n",
    "    for i in range(len_weights):\n",
    "        weight = 1/len_weights\n",
    "        weights[i] = weight\n",
    "    return weights\n",
    "\n",
    "def calculate_R(df_mapped, full):\n",
    "    max = 0\n",
    "    len_df_mapped = len(df_mapped)\n",
    "    count = len_df_mapped // 10\n",
    "    for i in range(len_df_mapped):\n",
    "        valid_avg = len(df_mapped) - 1\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r[\"+str(i+1)+\" on \"+str(len_df_mapped)+\"] - R = \"+str(max)+\". Remaining \"+str(count)+\" to accept this.\")\n",
    "        for j in range(i, len_df_mapped):\n",
    "            distance = np.linalg.norm(np.array(df_mapped.loc[i][:]) - np.array(df_mapped.loc[j][:]))\n",
    "            if distance > max:\n",
    "                if not (distance - 2. <= max and max < distance + 2.):\n",
    "                    valid_avg = len(df_mapped) - 1\n",
    "                    count = len_df_mapped // 10\n",
    "                max = distance\n",
    "            else:\n",
    "                if distance - 2. <= max and max < distance + 2.:\n",
    "                    valid_avg -= 1\n",
    "        if valid_avg > 0:\n",
    "            count -= 1\n",
    "            if count == 0 and not full:\n",
    "                return max\n",
    "    return max\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_percentage=80):\n",
    "    total_entries = len(dataset)\n",
    "    x = total_entries // 100 * train_percentage\n",
    "    remaining = total_entries - x\n",
    "    dataset.head(x).to_csv(\"Dataset/Productions/Train/\" + new_file + \".csv\", index=False) \n",
    "    dataset.tail(remaining).to_csv(\"Dataset/Productions/Test/\" + new_file + \".csv\", index=False) \n",
    "\n",
    "# Funzioni per aggiornare il valore dentro \"Last.txt\" che serve per differenziare i vari test.\n",
    "\n",
    "def get_last_ID(increase=False):\n",
    "    with open(\"Last.txt\") as opened:\n",
    "        a = str(opened.read())\n",
    "    if increase:\n",
    "        increase_ID()\n",
    "    return a\n",
    "\n",
    "def increase_ID():\n",
    "    actual = int(get_last_ID())\n",
    "    actual += 1\n",
    "    with open(\"Last.txt\", \"w\") as outfile:\n",
    "        outfile.write(str(actual))\n",
    "\n",
    "def reset_ID():\n",
    "    with open(\"Last.txt\", \"w\") as outfile:\n",
    "        outfile.write(str(1))\n",
    "\n",
    "# Da usare per caricare in un DataFrame pandas gli attributi da un json salvato in Dataset/MappedAttributes/name_dot_json\n",
    "def get_attributes_map_from_json(name, numeric=False, dataframe=df_complete):\n",
    "    with open(\"Dataset/MappedAttributes/\" + name + \".json\") as json_file:\n",
    "        return generate_attributes_map(map_json=json.load(json_file), dataframe=dataframe, numeric_dataset=numeric)\n",
    "\n",
    "# Conta il numero totale di attributi unici nella colonna \"Column\" della mappa degli attributi\n",
    "def get_count_attributes(attributes_map):\n",
    "    return len(attributes_map[\"Column\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Wrappers #\n",
    "############\n",
    "\n",
    "# Standardizza l'intero dataset - Inoltre, creo la attribute map, che contiene il valore di middle tale per cui, ogni valore oltre quella soglia, viene preso come positivo\n",
    "# Ritorna una attribute_map che contiene i threshold di ogni singolo attributo\n",
    "\n",
    "def standardize_dataset(dataset):\n",
    "    generate_attributes_map(dataframe=dataset, numeric_dataset=True, standardize=True)\n",
    "\n",
    "\n",
    "# Calcola i punti di mid per ogni attributo del dataframe\n",
    "def get_mid_thresholds(dataset):\n",
    "    return generate_attributes_map(dataframe=dataset, numeric_dataset=True, standardize=False)\n",
    "\n",
    "\n",
    "# Carica la mappa degli attributi - Nel caso di dataset numerici, genera la mappa dei threshold - Se non è presente una mappa in json, la crea; altrimenti ripristina quella già presente\n",
    "def get_attributes_map(name, numeric=False, dataframe=df_complete):\n",
    "    if not os.path.exists(\"Dataset/MappedAttributes/\" + name + \".json\"):\n",
    "        attributes_map = generate_attributes_map(dataframe=dataframe, save_json=True, name=name, numeric_dataset=numeric)       # Genera il json che mappa gli attributi - salva con il nome scelto in Dataset/MappedAttributes\n",
    "    else:\n",
    "        attributes_map = get_attributes_map_from_json(name, numeric=numeric)                                                    # Mappa degli attributi in pandas\n",
    "    return attributes_map\n",
    "\n",
    "# Crea un vettore dei pesi - Inizializzato a 0 || Inizializzato a 1/n per ogni i\n",
    "def get_weights(n_attributes, contain_target=True, init_zero=False):\n",
    "    return generate_weights(n_attributes, contain_target, init_zero)\n",
    "\n",
    "# Calcola la distanza massima tra ogni vettore del dataframe (COSTOSO)\n",
    "def get_R(dataframe, full=True):\n",
    "    return calculate_R(dataframe, full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparazione del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Dividi dataframe - Inizializza scegliendo un nome per il dataframe #\n",
    "######################################################################\n",
    "\n",
    "# Seleziona dataframe - BISOGNA AVERE UN DATASET che contenga anche l'attributo target\n",
    "new_file = \"firewall\"                                      # Nome del file in cui salvare il dataset scelto\n",
    "numeric_dataset = False\n",
    "standardize = False\n",
    "\n",
    "if standardize:\n",
    "    standardize_dataset(df_complete)\n",
    "\n",
    "split_dataset(df_complete)                                  # Dividi il dataset in due .csv - salva in Dataset/Productions/Train (e Test)\n",
    "\n",
    "attributes_map = get_attributes_map(name=new_file, numeric=numeric_dataset, dataframe=df_complete)\n",
    "attributes_number = get_count_attributes(attributes_map=attributes_map)\n",
    "\n",
    "root = \"Dataset/Productions/\"\n",
    "df_train = pd.read_csv(root+\"/Train/\"+new_file+\".csv\")      # Dataframe di TRAIN\n",
    "df_test = pd.read_csv(root+\"/Test/\"+new_file+\".csv\")        # Dataframe di TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(val):\n",
    "    if val >= 0:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "def count_targets(df_train, name_target):\n",
    "    df_copy = map_values(pd.DataFrame(df_train),attributes_map,numeric_dataset)\n",
    "    medium = get_medium(standardize, name_target)\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    for k in range(len(df_copy[name_target])):\n",
    "        if df_copy.at[k, name_target] >= medium:\n",
    "            positives += 1\n",
    "        else:\n",
    "            negatives += 1\n",
    "    print(\"Positivi:\", positives, \"- Negativi:\", negatives)\n",
    "    \n",
    "# Separa il dataframe dal target\n",
    "def get_dataframes_train(df_train=[], name_target=\"\", attributes_map=[]):\n",
    "    if len(attributes_map)==0 or len(df_train)==0 or name_target==\"\":\n",
    "        return \"error - bad call\"\n",
    "    df_mapped = map_values(df_train, attributes_map, numeric_values=numeric_dataset)    # Crea il dataset mappato - Metti dei valori al posto dei dati\n",
    "    df_target = df_mapped.pop(name_target)                                              # Estrai le labels (dopo aver mappato di dataframe)\n",
    "    return df_mapped, df_target\n",
    "\n",
    "# Calcola segno del target - Se non ho un dataset numerico, ho già il target mappato\n",
    "# Estraggo il segno del target, mettendo \"-\" se appartiene all'intervallo (-inf, medium)\n",
    "def get_target_sign(target, medium):                    \n",
    "    if not numeric_dataset:\n",
    "        return target\n",
    "    if target < medium:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Data la attribute map, controlla un input per vedere se appartiene alla classe positiva o negativa\n",
    "def is_wrong(w_sum, target, medium):\n",
    "    if not numeric_dataset:\n",
    "        return w_sum * target < 0\n",
    "    else:                                                       \n",
    "        y = get_target_sign(target, medium)\n",
    "        yhat = sign(w_sum)\n",
    "        if yhat * y < 0:                       # i valori da -inf a medium, sono da considerarsi negativi \n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Utilizza name_target per estrarre i range positivi e negativi di un target\n",
    "def get_medium(standardize, name_target):\n",
    "    if numeric_dataset:\n",
    "        if standardize:\n",
    "            medium = list(attributes_map.query(\"Column=='\"+name_target+\"' & Sign=='middle'\")[\"Attributes\"])[0][1]\n",
    "        else:\n",
    "            medium = list(attributes_map.query(\"Column=='\"+name_target+\"' & Sign=='middle'\")[\"Attributes\"])[0][0]\n",
    "    else:\n",
    "        return 0\n",
    "    return medium\n",
    "\n",
    "# Crea un dizionario con [k] = {[Weight_k], [bias], [c (num errori)]}\n",
    "# e una lista di coppie [epoca, num_errori] \n",
    "def train_model(df_mapped, df_target, weights, bias, medium):\n",
    "    perceptrons = {}\n",
    "    epochs_errors = []\n",
    "    k = 0\n",
    "    c = 0\n",
    "    for e in range(epochs):\n",
    "        num_errors = 0\n",
    "        for i in range(len(df_target)):\n",
    "            w_sum = np.dot(df_mapped.loc[i][:], weights) + bias\n",
    "            if is_wrong(w_sum, df_target[i], medium):\n",
    "                num_errors += 1\n",
    "                perceptrons[k] = [list(weights), bias, c]\n",
    "                c = 1\n",
    "                #norm = np.linalg.norm(weights)\n",
    "                for j in range(len(weights)):\n",
    "                    weights[j] = weights[j] + (get_target_sign(df_target[i], medium) * df_mapped.loc[i][j]) #/ norm\n",
    "                bias = get_target_sign(df_target[i], medium) * (R**2)\n",
    "                k += 1\n",
    "            else:\n",
    "                c += 1\n",
    "        epochs_errors.append([e, num_errors])\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write( \"\\rEpoch: \"+ str(e) + \" - Errors:\" + str(num_errors))\n",
    "    if len(perceptrons) == 0:\n",
    "        perceptrons[k] = [list(weights), bias, c]\n",
    "    return perceptrons, epochs_errors\n",
    "\n",
    "# Si passano i due dataframe di train e target, fa il train su quel dataset ed eventualmente lo salva\n",
    "# medium serve per i problemi di classificazione binaria su valori reali. Usare get_medium(standardize) per calcolarlo\n",
    "# E' possibile inserire l'indice\n",
    "def train_and_save_res(df_mapped, df_target, weights, bias, save=True, add_index=True):\n",
    "    perceptrons, epoch_errors = train_model(df_mapped, df_target, weights, bias, medium)                          # Esegui il training method\n",
    "    ind = \"\"\n",
    "    if save:\n",
    "        if add_index:\n",
    "            ind = \"_\"+get_last_ID(True)\n",
    "        json_perceptrons = \"Perceptrons/\"+new_file+ind+\".json\"        # Devo salvare la lista dei perceptrons e dei loro singoli pesi, oltre al peso che singolarmente hanno per fare la somma finale\n",
    "        epo_erro = \"Evidences/Train/\"+new_file+ind+\".json\"\n",
    "        \n",
    "        with open(json_perceptrons, \"w\") as outfile:\n",
    "            outfile.write(json.dumps(perceptrons))\n",
    "\n",
    "        with open(epo_erro, \"w\") as outfile:\n",
    "            outfile.write(json.dumps(epoch_errors))\n",
    "    return perceptrons, epoch_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleziona i parametri per il Train\n",
    "Inserisci in *name_target* scegliendo uno di quelli sopra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     class cap-shape cap-surface cap-color bruises odor gill-attachment  \\\n",
      "0        p         x           s         n       t    p               f   \n",
      "1        e         x           s         y       t    a               f   \n",
      "2        e         b           s         w       t    l               f   \n",
      "3        p         x           y         w       t    p               f   \n",
      "4        e         x           s         g       f    n               f   \n",
      "...    ...       ...         ...       ...     ...  ...             ...   \n",
      "6475     p         f           y         e       f    y               f   \n",
      "6476     p         x           s         n       f    f               f   \n",
      "6477     p         x           s         e       f    y               f   \n",
      "6478     p         x           s         e       f    s               f   \n",
      "6479     p         x           s         n       f    s               f   \n",
      "\n",
      "     gill-spacing gill-size gill-color  ... stalk-surface-below-ring  \\\n",
      "0               c         n          k  ...                        s   \n",
      "1               c         b          k  ...                        s   \n",
      "2               c         b          n  ...                        s   \n",
      "3               c         n          n  ...                        s   \n",
      "4               w         b          k  ...                        s   \n",
      "...           ...       ...        ...  ...                      ...   \n",
      "6475            c         n          b  ...                        k   \n",
      "6476            c         n          b  ...                        s   \n",
      "6477            c         n          b  ...                        s   \n",
      "6478            c         n          b  ...                        k   \n",
      "6479            c         n          b  ...                        k   \n",
      "\n",
      "     stalk-color-above-ring stalk-color-below-ring veil-type veil-color  \\\n",
      "0                         w                      w         p          w   \n",
      "1                         w                      w         p          w   \n",
      "2                         w                      w         p          w   \n",
      "3                         w                      w         p          w   \n",
      "4                         w                      w         p          w   \n",
      "...                     ...                    ...       ...        ...   \n",
      "6475                      w                      p         p          w   \n",
      "6476                      w                      p         p          w   \n",
      "6477                      w                      w         p          w   \n",
      "6478                      w                      w         p          w   \n",
      "6479                      p                      p         p          w   \n",
      "\n",
      "     ring-number ring-type spore-print-color population habitat  \n",
      "0              o         p                 k          s       u  \n",
      "1              o         p                 n          n       g  \n",
      "2              o         p                 n          n       m  \n",
      "3              o         p                 k          s       u  \n",
      "4              o         e                 n          a       g  \n",
      "...          ...       ...               ...        ...     ...  \n",
      "6475           o         e                 w          v       d  \n",
      "6476           o         e                 w          v       l  \n",
      "6477           o         e                 w          v       p  \n",
      "6478           o         e                 w          v       d  \n",
      "6479           o         e                 w          v       p  \n",
      "\n",
      "[6480 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# Stampa la lista degli attributi\n",
    "print(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_target = \"class\"                                       # Imposta il nome dell'attributo che fa da label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivi: 2779 - Negativi: 3701\n",
      "[648 on 6480] - R = 7.483314773547883. Remaining 1 to accept this..."
     ]
    }
   ],
   "source": [
    "# Dati da calcolare sul dataset (operazioni costose)\n",
    "count_targets(df_train, name_target)\n",
    "\n",
    "df_mapped, df_target = get_dataframes_train(df_train=df_train, name_target=name_target, attributes_map=attributes_map)\n",
    "\n",
    "R = 7.48331 #get_R(df_mapped, full=False)   # Alternativamente, imposta il valore se già conosciuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costanti per il perceptrons\n",
    "weights = get_weights(len(df_mapped.columns), contain_target=False, init_zero=True)\n",
    "bias = 1\n",
    "epochs = 30\n",
    "medium = get_medium(standardize, name_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 - Errors:21"
     ]
    }
   ],
   "source": [
    "perc, epc = train_and_save_res(df_mapped, df_target, weights, bias, save=True, add_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[   9.    5.  -11.   37. -111.   43.   93.  127.    9.  -41.   43.  -79.\n",
      "  -65.   29.   27.   43.   43.   23.   73.  -83.   47.   -9.]\n"
     ]
    }
   ],
   "source": [
    "print(medium)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_name = \"abalone\"\n",
    "index = \"_3\"\n",
    "\n",
    "with open(\"Perceptrons/\" + perceptron_name + index +\".json\") as json_file:\n",
    "    test_perceptrons = json.load(json_file)\n",
    "\n",
    "df_test_mapped = map_values(dataframe=df_test, attributes_map=attributes_map, numeric_values=numeric_dataset)   # Prende il dataframe da json\n",
    "df_test_target = df_test_mapped.pop(name_target)            # name_target è il nome della colonna che viene presa come target - selezionala dalla sezione di Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptrons from json : [0] lista pesi , [1] bias, [2] c (peso, ovvero numero di previsioni corrette)\n",
    "\n",
    "def predict(perceptrons_from_json, input_values):\n",
    "    average = 0.\n",
    "    voted = 0.\n",
    "    for i in perceptrons_from_json:\n",
    "        w_sum = np.dot(perceptrons_from_json[i][0], input_values)# + perceptrons_from_json[i][1]     # Fa il dot product. ovvero W * x\n",
    "        average += perceptrons_from_json[i][2] * w_sum                                              # Moltiplica per il peso\n",
    "        voted += perceptrons_from_json[i][2] * sign(w_sum)\n",
    "    return sign(average), sign(voted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# couple_avg_voted{k} [0] è il risultato di perceptron avg, [1] risultato di voted e [2] target \n",
    "couple_avg_voted = {}\n",
    "\n",
    "for k in range(len(df_test_mapped)):\n",
    "    couple_avg_voted[k] = list(predict(test_perceptrons, df_test_mapped.loc[k][:]))\n",
    "    couple_avg_voted[k].append(df_test_target[k])\n",
    "\n",
    "total_test_values = len(df_test_target)\n",
    "n_correct_avg = 0\n",
    "n_correct_vote = 0\n",
    "\n",
    "for k in couple_avg_voted:\n",
    "    if couple_avg_voted[k][0] == get_target_sign(couple_avg_voted[k][2], medium):\n",
    "        n_correct_avg += 1\n",
    "    if couple_avg_voted[k][1] == get_target_sign(couple_avg_voted[k][2], medium):\n",
    "        n_correct_vote += 1\n",
    "\n",
    "mistakes_avg = total_test_values - n_correct_avg\n",
    "mistakes_vote = total_test_values - n_correct_vote\n",
    "\n",
    "avg = {\"mistakes\" : mistakes_avg, \"correct\" : n_correct_avg, \"total\":total_test_values}\n",
    "vote = {\"mistakes\": mistakes_vote, \"correct\" : n_correct_vote, \"total\":total_test_values}\n",
    "\n",
    "str_mistakes_avg = \"Mistakes in avg: \" + str(mistakes_avg) + \" - Total correct: \" + str(n_correct_avg)\n",
    "str_mistakes_vote = \"Mistakes in voted: \" + str(mistakes_vote) + \" - Total correct: \" + str(n_correct_vote)\n",
    "\n",
    "with open(\"Evidences/Test/AVG/\"+perceptron_name+\".json\", \"w\") as opened:\n",
    "    opened.write(json.dumps(avg))\n",
    "\n",
    "with open(\"Evidences/Test/Vote/\"+perceptron_name+\".json\", \"w\") as opened:\n",
    "    opened.write(json.dumps(vote))\n",
    "\n",
    "print(str_mistakes_vote)\n",
    "print(str_mistakes_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(couple_avg_voted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione di grafici\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_train(name, time=\"\"):\n",
    "    x = []\n",
    "    y = []\n",
    "    if time != \"\":\n",
    "        id = \"_\"+ time\n",
    "    with open(\"Evidences/Train/\"+name+id+\".json\") as opened:\n",
    "        data = json.load(opened)\n",
    "    for k in data:\n",
    "        x.append(k[0])\n",
    "        y.append(k[1])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_x_y_train(\"abalone\", str(3))\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"N. Errors\")\n",
    "plt.title(perceptron_name + \" - Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Create Dataframe contenente PERC - Indovinate - Sbagliate\n",
    "printer = []\n",
    "\n",
    "printer.append(avg)\n",
    "printer.append(vote)\n",
    "\n",
    "pd.DataFrame(printer, index=[\"AVG\", \"Vote\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be3672e9201ffa1c176298987f4ba2c864b3318632361fa75d7c5a898e51e9a0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pythonProject': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
