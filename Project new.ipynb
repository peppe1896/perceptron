{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Perceptron votato*\n",
    "\n",
    "### Imports - Prime variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole-weight</th>\n",
       "      <th>Shucked-weight</th>\n",
       "      <th>Viscera-weight</th>\n",
       "      <th>Shell-weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>F</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>M</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>M</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>F</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>M</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sex  Length  Diameter  Height  Whole-weight  Shucked-weight  \\\n",
       "0      M   0.455     0.365   0.095        0.5140          0.2245   \n",
       "1      M   0.350     0.265   0.090        0.2255          0.0995   \n",
       "2      F   0.530     0.420   0.135        0.6770          0.2565   \n",
       "3      M   0.440     0.365   0.125        0.5160          0.2155   \n",
       "4      I   0.330     0.255   0.080        0.2050          0.0895   \n",
       "...   ..     ...       ...     ...           ...             ...   \n",
       "4172   F   0.565     0.450   0.165        0.8870          0.3700   \n",
       "4173   M   0.590     0.440   0.135        0.9660          0.4390   \n",
       "4174   M   0.600     0.475   0.205        1.1760          0.5255   \n",
       "4175   F   0.625     0.485   0.150        1.0945          0.5310   \n",
       "4176   M   0.710     0.555   0.195        1.9485          0.9455   \n",
       "\n",
       "      Viscera-weight  Shell-weight  Rings  \n",
       "0             0.1010        0.1500     15  \n",
       "1             0.0485        0.0700      7  \n",
       "2             0.1415        0.2100      9  \n",
       "3             0.1140        0.1550     10  \n",
       "4             0.0395        0.0550      7  \n",
       "...              ...           ...    ...  \n",
       "4172          0.2390        0.2490     11  \n",
       "4173          0.2145        0.2605     10  \n",
       "4174          0.2875        0.3080      9  \n",
       "4175          0.2610        0.2960     10  \n",
       "4176          0.3765        0.4950     12  \n",
       "\n",
       "[4177 rows x 9 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########\n",
    "# Imports #\n",
    "###########\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###################\n",
    "# Prime variabili #\n",
    "###################\n",
    "\n",
    "file_path = \"Dataset/abalone/abalone.data\"                 # Dataset\n",
    "df_complete = pd.read_csv(file_path)                # Seleziona CSV - Tutto il Dataset in file_path\n",
    "df_complete.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Length  Diameter  Height  Whole-weight  Shucked-weight  Viscera-weight  \\\n",
      "0      0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
      "1      0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
      "2      0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
      "3      0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
      "4      0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
      "...      ...       ...     ...           ...             ...             ...   \n",
      "4172   0.565     0.450   0.165        0.8870          0.3700          0.2390   \n",
      "4173   0.590     0.440   0.135        0.9660          0.4390          0.2145   \n",
      "4174   0.600     0.475   0.205        1.1760          0.5255          0.2875   \n",
      "4175   0.625     0.485   0.150        1.0945          0.5310          0.2610   \n",
      "4176   0.710     0.555   0.195        1.9485          0.9455          0.3765   \n",
      "\n",
      "      Shell-weight  Rings  \n",
      "0           0.1500     15  \n",
      "1           0.0700      7  \n",
      "2           0.2100      9  \n",
      "3           0.1550     10  \n",
      "4           0.0550      7  \n",
      "...            ...    ...  \n",
      "4172        0.2490     11  \n",
      "4173        0.2605     10  \n",
      "4174        0.3080      9  \n",
      "4175        0.2960     10  \n",
      "4176        0.4950     12  \n",
      "\n",
      "[4177 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Dataframe intero #\n",
    "####################\n",
    "columns_to_drop = [\"Sex\"]\n",
    "df_complete = df_complete.drop(columns=columns_to_drop)\n",
    "print(df_complete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funzioni - Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Funzioni per il dataset #\n",
    "###########################\n",
    "\n",
    "# Crea la mappa degli attributi, che serve per capire se un attributo è positivo o negativo\n",
    "# Se si passa map_json, vuol dire che si è già ricomposto un json precedentemente salvato, e quindi non serve ricrearlo\n",
    "# Una versione incapsulata è quella dopo, ovvero get_attribute_map_from_json\n",
    "\n",
    "def generate_attributes_map(map_json=[], dataframe=None, save_json=False, name=\"DELETABLE\", numeric_dataset=False, standardize=False):\n",
    "    map_of_attributes = map_json\n",
    "    if dataframe is None and len(map_of_attributes)==0:\n",
    "        return \"error - bad call\"\n",
    "    if len(map_of_attributes) == 0 :\n",
    "        for col in list(dataframe.columns.values):\n",
    "            if numeric_dataset:\n",
    "                column_to_array = dataframe[col].to_numpy()\n",
    "                max = float(dataframe[col].max())\n",
    "                min = float(dataframe[col].min())\n",
    "                mid_unstandardized = np.mean(column_to_array)                                   # (max + min) / 2\n",
    "                if standardize:\n",
    "                    mean = float(np.mean(column_to_array))\n",
    "                    std = float(np.std(column_to_array))\n",
    "                    for i in range(len(dataframe[col])):                                        # (dataframe.at[i, col] - mean) / std <- Standardiz.\n",
    "                        dataframe.at[i, col] = (dataframe.at[i, col]-min)/(max - min)           # <- Scalo in dataset\n",
    "                    new_max = float(dataframe[col].max())                                       # (dataframe.at[i, col]-min)/(max - min) <- Normalizzaz.\n",
    "                    new_min = float(dataframe[col].min())                \n",
    "                    mid = (new_max + new_min) / 2\n",
    "                else:\n",
    "                    mid = mid_unstandardized\n",
    "                map_of_attributes.append((str(col), \"middle\", [mid, mid_unstandardized]))       # Seleziono un mid per ogni attributo - magg o ugu a mid sono positivi\n",
    "            else:\n",
    "                attributes = df_complete[col].unique()\n",
    "                split = np.array_split(attributes, 2)\n",
    "                map_of_attributes.append((str(col), \"positive\", list(split[0])))\n",
    "                map_of_attributes.append((str(col), \"negative\", list(split[1])))\n",
    "    attributes_map = pd.DataFrame(map_of_attributes, columns=[\"Column\", \"Sign\", \"Attributes\"])\n",
    "\n",
    "    if save_json:\n",
    "        with open(\"Dataset/MappedAttributes/\"+name+\".json\", \"w\") as outfile:\n",
    "            outfile.write(json.dumps(map_of_attributes))\n",
    "    \n",
    "    return attributes_map\n",
    "\n",
    "# Crea un dataframe che contiene tutte le tuple di entry (ovvero tutti i vettori x) che servono per calcolare \n",
    "\n",
    "def map_continue_values(dataframe, attributes_map, standardized=False):\n",
    "    df_mapped = pd.DataFrame(columns=dataframe.columns)\n",
    "    for col in dataframe.columns:\n",
    "        if standardized:\n",
    "            positive = list(attributes_map.query(\"Column == '%s' & Sign == 'middle'\" %col )[\"Attributes\"])[0][1]\n",
    "        else:\n",
    "            positive = list(attributes_map.query(\"Column == '%s' & Sign == 'middle'\" %col )[\"Attributes\"])[0][0]\n",
    "        for i in range(len(dataframe)):\n",
    "            if dataframe.loc[i][col] < positive:\n",
    "                df_mapped.at[i, col] = -1\n",
    "            else:\n",
    "                df_mapped.at[i, col] = 1\n",
    "    return df_mapped\n",
    "\n",
    "def map_values(dataframe, attributes_map, numeric_values=False, standardized=False):\n",
    "    df_mapped = pd.DataFrame(columns=dataframe.columns)                                                             # Creo il nuovo dataset con l'insieme di attributi del dataframe originale\n",
    "    if not numeric_values:\n",
    "        for col in dataframe.columns:                                                                               # Per ogni attributo, guardo quale set è definito positivo\n",
    "            positives = list(attributes_map.query(\"Column == '%s' & Sign == 'positive'\" %col )[\"Attributes\"])[0]    # Query nel dataframe per trovare i positivi\n",
    "            negatives = list(attributes_map.query(\"Column == '%s' & Sign == 'negative'\" %col )[\"Attributes\"])[0]\n",
    "            for i in range(len(dataframe)):\n",
    "                if dataframe.loc[i][col] in positives:\n",
    "                    df_mapped.at[i, col] = 1\n",
    "                elif dataframe.loc[i][col] in negatives:\n",
    "                    df_mapped.at[i, col] = -1\n",
    "                else:\n",
    "                    df_mapped.at[i, col] = 0\n",
    "                    print(\"ERROR: This attribute isn't positive or negative!!\" + str(dataframe.loc[i][col]) + \"POSI: \" + str(positives) + \" - NEGA: \" + str(negatives) )\n",
    "                    return\n",
    "    else:\n",
    "        return map_continue_values(dataframe, attributes_map, standardized)    # Se sono valori numerici, sono stati già normalizzati in generate_attribute_map() - e gli intervalli + e - sono definiti\n",
    "    return df_mapped\n",
    "    \n",
    "def generate_weights(n_attributes, contain_target=True, init_zero=False):\n",
    "    attributes = n_attributes\n",
    "    if contain_target:\n",
    "        attributes -= 1\n",
    "    # attributes -= len(columns_to_drop)              # Bisogna rimuovere anche il numero di colonne rimosse\n",
    "    weights = np.zeros( attributes )\n",
    "    if init_zero:\n",
    "        return weights\n",
    "    len_weights = len(weights)\n",
    "    for i in range(len_weights):\n",
    "        weight = 1/len_weights\n",
    "        weights[i] = weight\n",
    "    return weights\n",
    "\n",
    "def calculate_R(df_mapped, full):\n",
    "    max = 0\n",
    "    len_df_mapped = len(df_mapped)\n",
    "    count = len_df_mapped // 10\n",
    "    for i in range(len_df_mapped):\n",
    "        valid_avg = len(df_mapped) - 1\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write(\"\\r[\"+str(i+1)+\" on \"+str(len_df_mapped)+\"] - R = \"+str(max)+\". Remaining \"+str(count)+\" to accept this.\")\n",
    "        for j in range(i, len_df_mapped):\n",
    "            distance = np.linalg.norm(np.array(df_mapped.loc[i][:]) - np.array(df_mapped.loc[j][:]))\n",
    "            if distance > max:\n",
    "                if not (distance - 2. <= max and max < distance + 2.):\n",
    "                    valid_avg = len(df_mapped) - 1\n",
    "                    count = len_df_mapped // 10\n",
    "                max = distance\n",
    "            else:\n",
    "                if distance - 2. <= max and max < distance + 2.:\n",
    "                    valid_avg -= 1\n",
    "        if valid_avg > 0:\n",
    "            count -= 1\n",
    "            if count == 0 and not full:\n",
    "                return max\n",
    "    return max\n",
    "\n",
    "\n",
    "def split_dataset(dataset, train_percentage=80):\n",
    "    total_entries = len(dataset)\n",
    "    x = total_entries // 100 * train_percentage\n",
    "    remaining = total_entries - x\n",
    "    dataset.head(x).to_csv(\"Dataset/Productions/Train/\" + new_file + \".csv\", index=False) \n",
    "    dataset.tail(remaining).to_csv(\"Dataset/Productions/Test/\" + new_file + \".csv\", index=False) \n",
    "\n",
    "# Funzioni per aggiornare il valore dentro \"Last.txt\" che serve per differenziare i vari test.\n",
    "\n",
    "def get_last_ID(increase=False):\n",
    "    with open(\"Last.txt\") as opened:\n",
    "        a = str(opened.read())\n",
    "    if increase:\n",
    "        increase_ID()\n",
    "    return a\n",
    "\n",
    "def increase_ID():\n",
    "    actual = int(get_last_ID())\n",
    "    actual += 1\n",
    "    with open(\"Last.txt\", \"w\") as outfile:\n",
    "        outfile.write(str(actual))\n",
    "\n",
    "def reset_ID():\n",
    "    with open(\"Last.txt\", \"w\") as outfile:\n",
    "        outfile.write(str(1))\n",
    "\n",
    "# Da usare per caricare in un DataFrame pandas gli attributi da un json salvato in Dataset/MappedAttributes/name_dot_json\n",
    "def get_attributes_map_from_json(name, numeric=False, dataframe=df_complete):\n",
    "    with open(\"Dataset/MappedAttributes/\" + name + \".json\") as json_file:\n",
    "        return generate_attributes_map(map_json=json.load(json_file), dataframe=dataframe, numeric_dataset=numeric)\n",
    "\n",
    "# Conta il numero totale di attributi unici nella colonna \"Column\" della mappa degli attributi\n",
    "def get_count_attributes(attributes_map):\n",
    "    return len(attributes_map[\"Column\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Wrappers #\n",
    "############\n",
    "\n",
    "# Standardizza l'intero dataset - Inoltre, creo la attribute map, che contiene il valore di middle tale per cui, ogni valore oltre quella soglia, viene preso come positivo\n",
    "# Ritorna una attribute_map che contiene i threshold di ogni singolo attributo\n",
    "\n",
    "def standardize_dataset(dataset):\n",
    "    generate_attributes_map(dataframe=dataset, numeric_dataset=True, standardize=True)\n",
    "\n",
    "\n",
    "# Calcola i punti di mid per ogni attributo del dataframe\n",
    "def get_mid_thresholds(dataset):\n",
    "    return generate_attributes_map(dataframe=dataset, numeric_dataset=True, standardize=False)\n",
    "\n",
    "\n",
    "# Carica la mappa degli attributi - Nel caso di dataset numerici, genera la mappa dei threshold - Se non è presente una mappa in json, la crea; altrimenti ripristina quella già presente\n",
    "def get_attributes_map(name, numeric=False, dataframe=df_complete):\n",
    "    if not os.path.exists(\"Dataset/MappedAttributes/\" + name + \".json\"):\n",
    "        attributes_map = generate_attributes_map(dataframe=dataframe, save_json=True, name=name, numeric_dataset=numeric)       # Genera il json che mappa gli attributi - salva con il nome scelto in Dataset/MappedAttributes\n",
    "    else:\n",
    "        attributes_map = get_attributes_map_from_json(name, numeric=numeric)                                                    # Mappa degli attributi in pandas\n",
    "    return attributes_map\n",
    "\n",
    "# Crea un vettore dei pesi - Inizializzato a 0 || Inizializzato a 1/n per ogni i\n",
    "def get_weights(n_attributes, contain_target=True, init_zero=False):\n",
    "    return generate_weights(n_attributes, contain_target, init_zero)\n",
    "\n",
    "# Calcola la distanza massima tra ogni vettore del dataframe (COSTOSO)\n",
    "def get_R(dataframe, full=True):\n",
    "    return calculate_R(dataframe, full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparazione del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Dividi dataframe - Inizializza scegliendo un nome per il dataframe #\n",
    "######################################################################\n",
    "\n",
    "# Seleziona dataframe - BISOGNA AVERE UN DATASET che contenga anche l'attributo target\n",
    "new_file = \"abalone\"                                       # Nome del file in cui salvare il dataset scelto\n",
    "numeric_dataset = True\n",
    "standardize = False\n",
    "\n",
    "if standardize:\n",
    "    standardize_dataset(df_complete)\n",
    "\n",
    "split_dataset(df_complete)                                  # Dividi il dataset in due .csv - salva in Dataset/Productions/Train (e Test)\n",
    "\n",
    "attributes_map = get_attributes_map(name=new_file, numeric=numeric_dataset, dataframe=df_complete)\n",
    "attributes_number = get_count_attributes(attributes_map=attributes_map)\n",
    "\n",
    "root = \"Dataset/Productions/\"\n",
    "df_train = pd.read_csv(root+\"/Train/\"+new_file+\".csv\")      # Dataframe di TRAIN\n",
    "df_test = pd.read_csv(root+\"/Test/\"+new_file+\".csv\")        # Dataframe di TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "### Funzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(val):\n",
    "    if val >= 0:\n",
    "        return 1\n",
    "    return -1\n",
    "\n",
    "def count_targets(df_train, name_target):\n",
    "    df_copy = map_values(pd.DataFrame(df_train),attributes_map,numeric_dataset, standardize)\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    for k in range(len(df_copy[name_target])):\n",
    "        if df_copy.at[k, name_target] == 1:\n",
    "            positives += 1\n",
    "        else:\n",
    "            negatives += 1\n",
    "    print(\"Positivi:\", positives, \"- Negativi:\", negatives)\n",
    "    \n",
    "# Separa il dataframe dal target\n",
    "def get_dataframes_train(df_train=[], name_target=\"\", attributes_map=[]):\n",
    "    if len(attributes_map)==0 or len(df_train)==0 or name_target==\"\":\n",
    "        return \"error - bad call\"\n",
    "    df_mapped = map_values(df_train, attributes_map, numeric_values=numeric_dataset, standardized=standardize)    # Crea il dataset mappato - Metti dei valori al posto dei dati\n",
    "    df_target = df_mapped.pop(name_target)                                              # Estrai le labels (dopo aver mappato di dataframe)\n",
    "    return df_mapped, df_target\n",
    "\n",
    "# Calcola segno del target - Se non ho un dataset numerico, ho già il target mappato\n",
    "# Estraggo il segno del target, mettendo \"-\" se appartiene all'intervallo (-inf, medium)\n",
    "def get_target_sign(target, medium):                    \n",
    "    if not numeric_dataset:\n",
    "        return target\n",
    "    if target < medium:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Data la attribute map, controlla un input per vedere se appartiene alla classe positiva o negativa\n",
    "def is_wrong(w_sum, target, medium):\n",
    "    if not numeric_dataset:\n",
    "        return w_sum * target < 0\n",
    "    else:                                                       \n",
    "        y = get_target_sign(target, medium)\n",
    "        yhat = sign(w_sum)\n",
    "        if yhat * y < 0:                       # i valori da -inf a medium, sono da considerarsi negativi \n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Utilizza name_target per estrarre i range positivi e negativi di un target\n",
    "def get_medium(standardize, name_target):\n",
    "    if numeric_dataset:\n",
    "        if standardize:\n",
    "            medium = list(attributes_map.query(\"Column=='\"+name_target+\"' & Sign=='middle'\")[\"Attributes\"])[0][1]\n",
    "        else:\n",
    "            medium = list(attributes_map.query(\"Column=='\"+name_target+\"' & Sign=='middle'\")[\"Attributes\"])[0][0]\n",
    "    else:\n",
    "        return 0            # Tutti gli attributi non numerici hanno come medium 0\n",
    "    return medium\n",
    "\n",
    "# Crea un dizionario con [k] = {[Weight_k], [bias], [c (num errori)]}\n",
    "# e una lista di coppie [epoca, num_errori] \n",
    "def train_model(df_mapped, df_target, weights, bias, medium):\n",
    "    perceptrons = {}\n",
    "    epochs_errors = []\n",
    "    k = 0\n",
    "    c = 0\n",
    "    for e in range(epochs):\n",
    "        num_errors = 0\n",
    "        for i in range(len(df_target)):\n",
    "            w_sum = np.dot(df_mapped.loc[i][:], weights) + bias\n",
    "            if is_wrong(w_sum, df_target[i], medium):\n",
    "                num_errors += 1\n",
    "                perceptrons[k] = [list(weights), bias, c]\n",
    "                c = 1\n",
    "                #norm = np.linalg.norm(weights)\n",
    "                for j in range(len(weights)):\n",
    "                    weights[j] = weights[j] + (get_target_sign(df_target[i], medium) * df_mapped.loc[i][j]) #/ norm\n",
    "                bias = get_target_sign(df_target[i], medium) * (R**2)\n",
    "                k += 1\n",
    "            else:\n",
    "                c += 1\n",
    "        epochs_errors.append([e, num_errors])\n",
    "        sys.stdout.flush()\n",
    "        sys.stdout.write( \"\\rEpoch: \"+ str(e) + \" - Errors:\" + str(num_errors))\n",
    "    if len(perceptrons) == 0:\n",
    "        perceptrons[k] = [list(weights), bias, c]\n",
    "    return perceptrons, epochs_errors\n",
    "\n",
    "# Si passano i due dataframe di train e target, fa il train su quel dataset ed eventualmente lo salva\n",
    "# medium serve per i problemi di classificazione binaria su valori reali. Usare get_medium(standardize) per calcolarlo\n",
    "# E' possibile inserire l'indice\n",
    "def train_and_save_res(df_mapped, df_target, weights, bias, save=True, add_index=True):\n",
    "    perceptrons, epoch_errors = train_model(df_mapped, df_target, weights, bias, medium)    # Esegui il training method\n",
    "    ind = \"\"\n",
    "    if save:\n",
    "        if add_index:\n",
    "            ind = \"_\"+get_last_ID(True)\n",
    "        json_perceptrons = \"Perceptrons/\"+new_file+ind+\".json\"        \n",
    "        epo_erro = \"Evidences/Train/\"+new_file+ind+\".json\"\n",
    "        \n",
    "        with open(json_perceptrons, \"w\") as outfile:\n",
    "            outfile.write(json.dumps(perceptrons))\n",
    "\n",
    "        with open(epo_erro, \"w\") as outfile:\n",
    "            outfile.write(json.dumps(epoch_errors))\n",
    "    return perceptrons, epoch_errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleziona i parametri per il Train\n",
    "Inserisci in *name_target* scegliendo uno di quelli sopra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Length', 'Diameter', 'Height', 'Whole-weight', 'Shucked-weight',\n",
      "       'Viscera-weight', 'Shell-weight', 'Rings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Stampa la lista degli attributi\n",
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_target = \"Rings\"                                       # Imposta il nome dell'attributo che fa da label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positivi: 1636 - Negativi: 1644\n"
     ]
    }
   ],
   "source": [
    "# Dati da calcolare sul dataset (operazioni costose)\n",
    "count_targets(df_train, name_target)\n",
    "\n",
    "df_features, df_target = get_dataframes_train(df_train=df_train, name_target=name_target, attributes_map=attributes_map)\n",
    "\n",
    "R = 5.291502622129181 #get_R(df_features, full=False)   # Alternativamente, imposta il valore se già conosciuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Length Diameter Height Whole-weight Shucked-weight Viscera-weight  \\\n",
      "0        -1       -1     -1           -1             -1             -1   \n",
      "1        -1       -1     -1           -1             -1             -1   \n",
      "2         1        1     -1           -1             -1             -1   \n",
      "3        -1       -1     -1           -1             -1             -1   \n",
      "4        -1       -1     -1           -1             -1             -1   \n",
      "...     ...      ...    ...          ...            ...            ...   \n",
      "3275     -1       -1     -1           -1             -1             -1   \n",
      "3276     -1       -1     -1           -1             -1             -1   \n",
      "3277     -1       -1      1           -1             -1             -1   \n",
      "3278      1        1      1           -1             -1             -1   \n",
      "3279      1        1      1            1              1              1   \n",
      "\n",
      "     Shell-weight  \n",
      "0              -1  \n",
      "1              -1  \n",
      "2              -1  \n",
      "3              -1  \n",
      "4              -1  \n",
      "...           ...  \n",
      "3275           -1  \n",
      "3276           -1  \n",
      "3277           -1  \n",
      "3278           -1  \n",
      "3279            1  \n",
      "\n",
      "[3280 rows x 7 columns]\n",
      "0        1\n",
      "1       -1\n",
      "2       -1\n",
      "3        1\n",
      "4       -1\n",
      "        ..\n",
      "3275     1\n",
      "3276     1\n",
      "3277     1\n",
      "3278     1\n",
      "3279     1\n",
      "Name: Rings, Length: 3280, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_features)\n",
    "print(df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Costanti per il perceptrons\n",
    "weights = get_weights(len(df_features.columns), contain_target=False, init_zero=True)\n",
    "bias = 1\n",
    "epochs = 30\n",
    "medium = get_medium(standardize, name_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 - Errors:844"
     ]
    }
   ],
   "source": [
    "perc, epc = train_and_save_res(df_features, df_target, weights, bias, save=True, add_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 8.  6. 12. -2. -6. -2. 12.]\n"
     ]
    }
   ],
   "source": [
    "print(medium)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perceptron_name = \"abalone\"\n",
    "index = \"_3\"\n",
    "\n",
    "with open(\"Perceptrons/\" + perceptron_name + index +\".json\") as json_file:\n",
    "    test_perceptrons = json.load(json_file)\n",
    "\n",
    "df_test_mapped = map_values(dataframe=df_test, attributes_map=attributes_map, numeric_values=numeric_dataset)   # Prende il dataframe da json\n",
    "df_test_target = df_test_mapped.pop(name_target)            # name_target è il nome della colonna che viene presa come target - selezionala dalla sezione di Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptrons from json : [0] lista pesi , [1] bias, [2] c (peso, ovvero numero di previsioni corrette)\n",
    "\n",
    "def predict(perceptrons_from_json, input_values):\n",
    "    average = 0.\n",
    "    voted = 0.\n",
    "    for i in perceptrons_from_json:\n",
    "        w_sum = np.dot(perceptrons_from_json[i][0], input_values)# + perceptrons_from_json[i][1]     # Fa il dot product. ovvero W * x\n",
    "        average += perceptrons_from_json[i][2] * w_sum                                              # Moltiplica per il peso\n",
    "        voted += perceptrons_from_json[i][2] * sign(w_sum)\n",
    "    return sign(average), sign(voted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# couple_avg_voted{k} [0] è il risultato di perceptron avg, [1] risultato di voted e [2] target \n",
    "couple_avg_voted = {}\n",
    "\n",
    "for k in range(len(df_test_mapped)):\n",
    "    couple_avg_voted[k] = list(predict(test_perceptrons, df_test_mapped.loc[k][:]))\n",
    "    couple_avg_voted[k].append(df_test_target[k])\n",
    "\n",
    "total_test_values = len(df_test_target)\n",
    "n_correct_avg = 0\n",
    "n_correct_vote = 0\n",
    "\n",
    "for k in couple_avg_voted:\n",
    "    if couple_avg_voted[k][0] == get_target_sign(couple_avg_voted[k][2], medium):\n",
    "        n_correct_avg += 1\n",
    "    if couple_avg_voted[k][1] == get_target_sign(couple_avg_voted[k][2], medium):\n",
    "        n_correct_vote += 1\n",
    "\n",
    "mistakes_avg = total_test_values - n_correct_avg\n",
    "mistakes_vote = total_test_values - n_correct_vote\n",
    "\n",
    "avg = {\"mistakes\" : mistakes_avg, \"correct\" : n_correct_avg, \"total\":total_test_values}\n",
    "vote = {\"mistakes\": mistakes_vote, \"correct\" : n_correct_vote, \"total\":total_test_values}\n",
    "\n",
    "str_mistakes_avg = \"Mistakes in avg: \" + str(mistakes_avg) + \" - Total correct: \" + str(n_correct_avg)\n",
    "str_mistakes_vote = \"Mistakes in voted: \" + str(mistakes_vote) + \" - Total correct: \" + str(n_correct_vote)\n",
    "\n",
    "with open(\"Evidences/Test/AVG/\"+perceptron_name+\".json\", \"w\") as opened:\n",
    "    opened.write(json.dumps(avg))\n",
    "\n",
    "with open(\"Evidences/Test/Vote/\"+perceptron_name+\".json\", \"w\") as opened:\n",
    "    opened.write(json.dumps(vote))\n",
    "\n",
    "print(str_mistakes_vote)\n",
    "print(str_mistakes_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(couple_avg_voted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creazione di grafici\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y_train(name, time=\"\"):\n",
    "    x = []\n",
    "    y = []\n",
    "    if time != \"\":\n",
    "        id = \"_\"+ time\n",
    "    with open(\"Evidences/Train/\"+name+id+\".json\") as opened:\n",
    "        data = json.load(opened)\n",
    "    for k in data:\n",
    "        x.append(k[0])\n",
    "        y.append(k[1])\n",
    "    return x, y\n",
    "\n",
    "x, y = get_x_y_train(\"abalone\", str(3))\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"N. Errors\")\n",
    "plt.title(perceptron_name + \" - Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Create Dataframe contenente PERC - Indovinate - Sbagliate\n",
    "printer = []\n",
    "\n",
    "printer.append(avg)\n",
    "printer.append(vote)\n",
    "\n",
    "pd.DataFrame(printer, index=[\"AVG\", \"Vote\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be3672e9201ffa1c176298987f4ba2c864b3318632361fa75d7c5a898e51e9a0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pythonProject': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
